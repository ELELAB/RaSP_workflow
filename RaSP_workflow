#!/usr/bin/env python3

#Rapid protein stability prediction, RaSP, Developed by Blaabjerg et al. 2022 doi: 10.1101/2022.07.14.500157 
#Adapted and implemented in 2022, Kristine Degn

#This file is an adapation and combination of: 
#   src/run_pipeline.py
#   the code presented in the google colab (https://colab.research.google.com/github/KULL-Centre/papers/blob/main/2022/ML-ddG-Blaabjerg-et-al/RaSPLab.ipynb). 
#   src/pdb_parser_scripts/parse_pdbs_pred.sh

# Summary of changes: 
#   Addition of argparse flags to ease the use of RaSP from the commandline including posibility of limitation on core usage
#   File naming convention
#   Folder structure, input and output placement of files.

import argparse
import shutil

# prepare parameters
parser = argparse.ArgumentParser(prog = "RaSP_workflow",
                                 usage = "RaSP_workflow [-h] help.\n Rapid protein stability prediction, RaSP\n Tool developed by Blaabjerg et al. 2022 doi: 10.1101/2022.07.14.500157 \n Adapted and implemented in 2022, Kristine Degn")

parser.add_argument("-t", "--type",
                    metavar = "the type of input file; FILE, PDB, AF",
                    type=str,
                    default="FILE",
                    help="RaSP can handle existing files in pdb format, FILE, a four digit PDB id, PDB. And an Alphafold code corresponding to the uniprot id. A FILE is default")

parser.add_argument("-i", '--input',
                    metavar = 'input pdb or code',
                    required=True,
                    type=str,
                    help='The name of the input pdb, either a file or a AF (uniprot) or PDB code')

parser.add_argument("-c", '--chain',
                    metavar = 'chain for saturation mutagenesis',
                    type=str,
                    required=True,
                    help='The chain to be analyzed')

parser.add_argument("-v", '--version',
                    metavar = 'version of the AlphaFold model to be analyzed',
                    type=int,
                    default=4,
                    help='The AF version to be analyzed')

parser.add_argument("-r", "--runtype",
                    metavar = "cpu or cuda",
                    type=str,
                    default = 'cpu',
                    help="RaSP can run on either cuda or cpu")

parser.add_argument("-p", "--path",
                    metavar = "Path to the src directory",
                    type=str,
                    default="./src/",
                    help="Add the path for the RaSP src directory if different from current working dir")

parser.add_argument("-o", "--output",
                    metavar = "Path to output",
                    type=str,
                    default=".",
                    help="Add the path for the output, default current working dir")

parser.add_argument("-n", "--cores",
                    metavar = "capping the number of cores to be used",
                    type=str,
                    default="1",
                    help="choose the cores for the RaSP scripts")
# to be updated based on new location

args = parser.parse_args()

import sys
import os
import glob
import random
import subprocess
cores = str(args.cores)
os.environ["OMP_NUM_THREADS"] = cores
import numpy as np
import pandas as pd
import torch
import time
import datetime
from Bio.PDB.Polypeptide import index_to_one, one_to_index

pdb_type = args.type
pdb_input = args.input
output = args.output
path = args.path
sys.path.append(path)

from cavity_model import (
     CavityModel,
     DownstreamModel,
     ResidueEnvironmentsDataset,
)

from helpers import (
     init_lin_weights,
     ds_pred,
     cavity_to_prism,
     get_seq_from_variant,
)

version = int(args.version)

chain = args.chain
uniquechain = chain

runtype = args.runtype

# Setup pipeline with parameters
## Set seeds
np.random.seed(0)
random.seed(0)
torch.manual_seed(0)
if runtype == 'cuda':
    torch.cuda.manual_seed(0)
    torch.cuda.manual_seed_all(0)
torch.backends.cudnn.enabled = False
torch.backends.cudnn.benchmark = False
torch.backends.cudnn.deterministic = True

## Main deep parameters
DEVICE = runtype # "cpu" or "cuda"
NUM_ENSEMBLE = 10
TASK_ID = int(1)
PER_TASK = int(1)

#create folder structure:

if not os.path.exists(f'{output}/data'): 
    os.makedirs(f'{output}/data')
if not os.path.exists(f'{output}/data/predictions'):
    os.makedirs(f'{output}/data/predictions')
if not os.path.exists(f'{output}/data/predictions/raw'):
    os.makedirs(f'{output}/data/predictions/raw')
if not os.path.exists(f'{output}/data/predictions/cleaned'):
    os.makedirs(f'{output}/data/predictions/cleaned')
if not os.path.exists(f'{output}/data/predictions/parsed'):
    os.makedirs(f'{output}/data/predictions/parsed')
if not os.path.exists(f'{output}/data/input'): 
    os.makedirs(f'{output}/data/input')

#load pdb
query_protein = f"{pdb_input}"

if pdb_type == "AF":
    process = subprocess.call(['curl','-s','-f',f'https://alphafold.ebi.ac.uk/files/AF-{pdb_input}-F1-model_v{version}.pdb','-o', f'{output}/data/input/{pdb_input}.pdb']) #'query_protein.pdb'])
    if process != 22:
        print("ALPHAFOLD downloaded")
    else:
        sys.exit('ERROR: Choosen type, AF, and input does not match')
elif pdb_type == "PDB":
    process = subprocess.call(['curl','-s','-f',f'https://files.rcsb.org/download/{pdb_input}.pdb','-o', f'{output}/data/input/{pdb_input}.pdb'])#'query_protein.pdb'])
    if process != 22:
        print("PDB downloaded")
    else:
        sys.exit('ERROR: Choosen type, PDB, and input does not match')
elif pdb_type == "FILE":
    if os.path.exists(f'{output}/data/input/{pdb_input}.pdb'):
        print('PDB file correctly loaded')
    elif os.path.exists(f'{pdb_input}.pdb'):
        shutil.copyfile(f"{pdb_input}.pdb", f"{output}/data/input/{pdb_input}.pdb")
        print('PDB file correctly loaded')
    else:
        sys.exit('PDB upload failed, check naming and placement of file')
else:
    sys.exit('ERROR: Choosen type is not supported, choose FILE, AF or PDB.')

os.system(f"pdb_validate {output}/data/input/{query_protein}.pdb > {output}/data/input/quality_control.txt")
with open(f"{output}/data/input/quality_control.txt", "r") as f:
    lines = f.readlines()

if 'It *seems* everything is OK.' not in lines:
    print(f"There seems to be a challenge with your input PDB, see {output}/data/input/quality_control.txt for details")
    exit(1)    

os.system(f"pdb_selchain -{chain} {output}/data/input/{query_protein}.pdb | pdb_delhetatm | pdb_delres --999:0:1 | pdb_fixinsert | pdb_tidy  > {output}/data/predictions/raw/{query_protein}_{uniquechain}.pdb")

pdb_input_dir = f'{output}/data/predictions/raw/'
input_pdbs = sorted(list(filter(lambda x: x.endswith(".pdb"), os.listdir(f'{output}/data/predictions/raw/'))))
start = (TASK_ID-1)*(PER_TASK)
end = (TASK_ID*PER_TASK)
if end > len(input_pdbs):
    end = len(input_pdbs) #avoid end index exceeding length of list
pdbs = input_pdbs[start:end] 
pdb_names = [i.split(".")[0] for i in pdbs]
print("Pre-processing PDBs ...")

os.system(f"python {path}/pdb_parser_scripts/clean_pdb.py --pdb_file_in {output}/data/predictions/raw/{query_protein}_{uniquechain}.pdb --out_dir {output}/data/predictions/cleaned/ --reduce_exe {path}/pdb_parser_scripts/reduce/reduce --path {path}") #&> /dev/null
os.system(f"python {path}/pdb_parser_scripts/extract_environments.py --pdb_in {output}/data/predictions/cleaned/{query_protein}_{uniquechain}_clean.pdb  --out_dir {output}/data/predictions/parsed/")  #&> /dev/null

if os.path.exists(f"{output}/data/predictions/parsed/{query_protein}_{uniquechain}_clean_coordinate_features.npz"):
  print("Pre-processing PDBs correctly ended")
else:
  print("Pre-processing PDB didn't end correcly, please check input informations")


print("#################### PREPARE RUN #####################")

pdb_filenames_ds = sorted(glob.glob(f"{output}/data/predictions/parsed/{query_protein}_{uniquechain}_clean_coordinate_features.npz"))

dataset_structure = ResidueEnvironmentsDataset(pdb_filenames_ds, transformer=None)

resenv_dataset = {}

for resenv in dataset_structure:
    key = (f"--{query_protein}--{resenv.chain_id}--{resenv.pdb_residue_number}--{index_to_one(resenv.restype_index)}--"
        )
    resenv_dataset[key] = resenv

    
df_structure_no_mt = pd.DataFrame.from_dict(resenv_dataset, orient='index', columns=["resenv"])
df_structure_no_mt.reset_index(inplace=True)
df_structure_no_mt["index"]=df_structure_no_mt["index"].astype(str)
res_info = pd.DataFrame(df_structure_no_mt["index"].str.split('--').tolist(),
                        columns = ['blank','pdb_id','chain_id','pos','wt_AA', 'blank2'])

df_structure_no_mt["pdbid"] = res_info['pdb_id']
df_structure_no_mt["chainid"] = res_info['chain_id']
df_structure_no_mt["variant"] = res_info["wt_AA"] + res_info['pos'] + "X"
aa_list = ["A", "C", "D", "E", "F", "G", "H", "I", "K", "L", 
            "M", "N", "P", "Q", "R", "S", "T", "V", "W", "Y"]
df_structure = pd.DataFrame(df_structure_no_mt.values.repeat(20, axis=0), columns=df_structure_no_mt.columns)
for i in range(0, len(df_structure), 20):
    for j in range(20):
        df_structure.iloc[i+j, :]["variant"] = df_structure.iloc[i+j, :]["variant"][:-1] + aa_list[j]
df_structure.drop(columns="index", inplace=True)


# Load PDB amino acid frequencies used to approximate unfolded states
pdb_nlfs = -np.log(np.load(f"{path}/pdb_frequencies.npz")["frequencies"])

# # Add wt and mt idxs and freqs to df
df_structure["wt_idx"] = df_structure.apply(lambda row: one_to_index(row["variant"][0]), axis=1)
df_structure["mt_idx"] = df_structure.apply(lambda row: one_to_index(row["variant"][-1]), axis=1)
df_structure["wt_nlf"] = df_structure.apply(lambda row: pdb_nlfs[row["wt_idx"]], axis=1)
df_structure["mt_nlf"] = df_structure.apply(lambda row: pdb_nlfs[row["mt_idx"]], axis=1)

# Define models
best_cavity_model_path = open(f"{path}/cavity_models/best_model_path.txt", "r").read()
#added line
best_cavity_model_path = best_cavity_model_path.split("/content/output/")[1][:-1]
best_cavity_model_path = f"{path}/{best_cavity_model_path}"

print("best cavity model path")
print(best_cavity_model_path)

cavity_model_net = CavityModel(get_latent=True).to(DEVICE)

if runtype == 'cpu':
    t = torch.load(f"{best_cavity_model_path}", map_location=torch.device('cpu'))
elif runtype == 'cuda':
    t = torch.load(f"{best_cavity_model_path}")

cavity_model_net.load_state_dict(t)

cavity_model_net.eval()
ds_model_net = DownstreamModel().to(DEVICE)
ds_model_net.apply(init_lin_weights)
ds_model_net.eval()

###set start time
start_time = time.perf_counter()

# Make ML predictions
print("Starting downstream model prediction")

print("########### RUNNING ################")
dataset_key="predictions"
df_ml = ds_pred(cavity_model_net,
                ds_model_net,
                df_structure,
                dataset_key,
                NUM_ENSEMBLE, #10
                DEVICE, #cpu
                path,
                ) 

print("Finished downstream model prediction")
end_time = time.perf_counter()
elapsed = datetime.timedelta(seconds = end_time - start_time)
print("Complete - prediction execution took", elapsed)

elapsed = datetime.timedelta(seconds = end_time - start_time)
print("Generating output files")
#Merge and save data with predictions

df_total = df_structure.merge(df_ml, on=['pdbid','chainid','variant'], how='outer')
df_total = df_total.drop("resenv", axis=1)
print(f"{len(df_structure)-len(df_ml)} data points dropped when matching total data with ml predictions in: {dataset_key}.")

if not os.path.exists(f'{output}/output'): 
    os.makedirs(f'{output}/output')
if not os.path.exists(f'{output}/output/{dataset_key}'): 
    os.makedirs(f'{output}/output/{dataset_key}')
    
# Parse output into separate files by pdb, print to PRISM format
for pdbid in df_total["pdbid"].unique():
    df_pdb = df_total[df_total["pdbid"]==pdbid]
    for chainid in df_pdb["chainid"].unique():
        pred_outfile = f"{output}/output/{dataset_key}/cavity_pred_{pdbid}_{chainid}.csv"
        print(f"Parsing predictions from pdb: {pdbid}{chainid} into {pred_outfile}")
        df_chain = df_pdb[df_pdb["chainid"]==chainid]
        df_chain = df_chain.assign(pos = df_chain["variant"].str[1:-1])
        df_chain['pos'] = pd.to_numeric(df_chain['pos'])
        first_res_no = min(df_chain["pos"])
        df_chain = df_chain.assign(wt_AA = df_chain["variant"].str[0])
        df_chain = df_chain.assign(mt_AA = df_chain["variant"].str[-1])
        seq = get_seq_from_variant(df_chain)
        df_chain.to_csv(pred_outfile, index=False)

# End timer and print result
elapsed = datetime.timedelta(seconds = end_time - start_time)

print("Complete - files generated")
